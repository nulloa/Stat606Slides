%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx, copyrightbox, bm, amsmath,verbatim}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\makeatletter
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Stat 606]{GPU usage in Spatial Models} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Nehemias Ulloa} % Your name
\institute[ISU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Iowa State University \\ % Your institution for the title page
\medskip
\textit{} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{GPU}
%------------------------------------------------

\begin{frame}
\begin{itemize}
\item GPU: Graphics Processing Unit
\item Chips/Processors similar to a CPU but they specialize in graphics
\item First appeared in arcade games
\item Motivation is largely still the same
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\begin{itemize}
\item What's the difference from a CPU?
\item Difference is in structure
\item GPU has a structure that is made for parrellelization
\item ``GPUs are optimized for taking huge batches of data and performing the same operation over and over very quickly, unlike PC microprocessors, which tend to skip all over the place'' - Nathan Brookwood
\end{itemize}


\copyrightbox[r]{\includegraphics[height=0.5\textheight]{gputech_f2.png}}{Source: NVIDIA}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{itemize}
\item Most common way to use a GPU is by using NVIDIA's \verb|CUDA| programming model 
\item Send work to the GPU using a {\it kernal} function
\item To write a good kernal function, need to understand the architecture of the GPU. 
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Here are the big components of the architecture:
\begin{itemize}
\item Threads - Smallest unit that executes a command 
\item Blocks - Group of 1024 threads per block
\item Kernal Grid - Group of 65,533 Blocks where a kernal fn is invoked 
\item Warp - Groups of threads in a block that simmultaneously execute a command
\item Streaming Multiprocessor(SM) - Wraps for the same block are same SM aka actually runs the CUDA kernals
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{center}
\copyrightbox{\includegraphics[height=0.7\textheight]{cudasoftware.png}}{Source:NVIDIA CUDA Compute Unified Device Architecture Programming Guide Version 1.1}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
We would like to run as many blocks as possible on the SM but that depends on two constraints:
\begin{itemize}
\item Amount of memory - Different for each card
\item Number of registers required by each thread - We have some control \\~\\
\end{itemize}

Ultimately the number of SMs and blocks per SM is defined by the hardware of the GPU. In the paper's example their card had 16 SMs and allowed multiple blocks per SM. \\
\end{frame}

%------------------------------------------------

\begin{frame}
Memory types and locations: \\

\begin{table}
\begin{tabular}{l | c | c}
\hline
Type         & Access   & Size  \\
\hline
Thread/Local & Thread   & Small \\
Shared       & Block    & 48 KB \\
Global       & Everyone & 5 GB   \\
\hline
\end{tabular}
\end{table}

For ``good'' GPU usage we want to:
\begin{itemize}
  \item Minimize need to access the Global Memory
  \item If possible `live' in the Thread/Local Memory
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{center}
\copyrightbox{\includegraphics[height=0.7\textheight]{gpumemory.png}}{Source:NVIDIA CUDA Compute Unified Device Architecture Programming Guide Version 1.1}
\end{center}
\end{frame}

%------------------------------------------------








%------------------------------------------------
\section{Examples}
%------------------------------------------------


%------------------------------------------------
\subsection{Spatial Models with Block Composite Likelihood}
%------------------------------------------------

\begin{frame}[fragile]

Geostatistical model:
\begin{align}
Y(\bm{s}) = \bm{x}^t(\bm{s})\bm{\beta} + w(\bm{s}) + \epsilon(\bm{s})
\end{align}

where 
\begin{itemize}
\item $\bm{s} \in \bm{D}  \subseteq \Re^d$ are spatial locations in 2 or 3 dim space \\
\item $Y(\bm{s})$ Guassian response variable \\
\item $\bm{x}^t(\bm{s})$ is a vector of explanatory variables \\
\item $\epsilon(\bm{s}) \distras{ind} N(0, \tau^2)$ \\
\item $w(\bm{s})$ provides the structural dependence i.e. the covariance structure
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Consider the data at $n$ loacations then we can consider our model like this:
\begin{gather*}
\bm{Y} \sim N(\bm{X\beta},\Sigma) \\
\text{where } \Sigma = \Sigma(\bm{\theta}) = \bm{C} + \tau^2 \bm{I}_n \\
\text{and } C(i,j) = cov(w(\bm{s}_i),w(\bm{s}_j))
\end{gather*}

Then we can consider the log-likelihood as:
\begin{align}
\ell (\bm{Y;\beta,\theta}) = -\frac{1}{2}log|\sigma| - \frac{1}{2}(\bm{Y} - \bm{X\beta})^{t}\Sigma^{-1}(\bm{Y} - \bm{X\beta})
\end{align}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{itemize}
\item As usual, the most computationally intensive parts will deal with computing $|\Sigma|$ and $\Sigma^{-1}$. One way to handle this is a composite likelihood based on pairwise data differences

\item This is a natural compromise for geostatistical models; the blocks allow us a good tradeoff between computational and statistical efficiency

\item The basic idea is to split our sample space ($\bm{D}$) into $M$ blocks where $\bigcup_k \bm{D}_k = \bm{D}$ and $\bm{D}_k \bigcap \bm{D}_l = \emptyset$
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
We can get the likelihood to look like: 

\begin{equation} \label{eq1}
\begin{split}
\ell(\bm{Y};\bm{\beta},\bm{\theta}) & = \sum_{k=1}^{M-1} \sum_{l>k} \ell(\bm{Y}_;\bm{\beta},\bm{\theta}) \\
                                    & = \sum_{k=1}^{M-1} \sum_{l>k} \big[ -\frac{1}{2}\log|\Sigma_{kl}| -\frac{1}{2}(\bm{Y_{kl}} - \bm{X_{kl}\beta})^{t}\Sigma^{-1}_{kl}(\bm{Y_{kl}} - \bm{X_{kl}\beta}) \big]
\end{split}
\end{equation}

where
\begin{itemize}
\item $\bm{Y}_k = \{ Y(\bm{s}_i):\bm{s}_i \in \bm{D}_k \}$
\item $\bm{Y}_{kl} = (\bm{Y}^t_k, \bm{Y}^t_l)^t$
\item $\bm{X}_{kl} = (\bm{X}^t_k, \bm{X}^t_l)^t$
\item $\bm{\Sigma}_{kl} = $\[
\begin{bmatrix}
    \bm{\Sigma}_{kl}(1,1)  & \bm{\Sigma}_{kl}(1,2) \\
    \bm{\Sigma}_{kl}(2,1)  & \bm{\Sigma}_{kl}(2,2)
\end{bmatrix}
\]
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
In order for this to work well we need to choose blocks carefully. The goal is to minimize the correlation between observations not in a block-pair and maximize the number of blocks simultaneously. \\~\\

Here are some ideas:
\begin{itemize}
\item Block widths equal to the effective spatial range
\item If interested in computation, use equally numbered blocks
\item Block according to spatial dependence
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Computationally, the Block Composite Likelihood complexity is of $O(n)$.\\~\\

The limit on memory is not an issue since the CL, score, and Hessian calculations are summations over independent calculations for each pair. Since these are independent calculations, they can be parrallelized. 
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Here are some sample results.\\

\begin{figure}[h]
\centering
\includegraphics[height=0.40\textheight]{gpucpucomparepap1.png}
\caption{This graphics compares CPU-only $\&$ GPU-only times for ALC calculations at $n_{cand}=60000$ ($\sim$ max number of blocks) candidate locations while changing $n$ (sample size).}
\end{figure}
\end{frame}


%------------------------------------------------
\subsection{Gaussian Process Regression}
%------------------------------------------------

\begin{frame}[fragile]

{\bf Def:} Gaussian process a group of random variables where any
subset of them have a joint Gaussian distribution
\begin{itemize}
\item Extend multivariate Gaussian distributions to infinite dimensionality e.g. $\bm{y} =  \{ y_1 , \ldots , y_n \}$ as a single point from a $n$-dim Multivariate Normal
\item Fully defined by a mean function and a covariance function
\item GP Regression is often used in computer emulation i.e. a distribution $Y(x)|D_N$ for new $x$ given simulated data $D_N$ $(p(y(x)|D_N, K_\theta))$
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
We typically assume that the mean function is zero, but there are multiple options for the covariance function. Some popular options are:
\begin{itemize}
\item Squared exponential
\item Matern
\item Noise (i.e. White noise - flat spectrum)
\end{itemize}

The papers used an isotropic Gaussian correlation structure:
\begin{align*}
K_{\theta, \eta}(x,x') = exp\{ -\parallel x-x' \parallel ^2 / \theta \}
\end{align*}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{itemize}
\item Problem is that inverse and determinant computations of the correlation matrix $K(,)$ are $O(N^3)$ 
\item One of the most common ways to handle this is to use sparsity
\item Gramacy and Apley(2014) sparsity scheme for accurate and fast predictions
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Here is the intuition behind the scheme:

\begin{itemize}
\item Focus on the prediction problem at some $x$
\item Data far away from prediction point won't have much influence
\item Use a subdesign $D_n(x) \equiv D_n(X_n(x))$
\item Most common option is Nearest Neighbors(NN), but it has issues
      \begin{itemize}
      \item Stein, Chi, Welty (2004) showed that using a few points away from the point of interest improves prediction and parameter estimation
      \end{itemize}
\item A criteria that cycles through possible $D_n(x)$ until optimal is found
\item This is done with no extra computational cost compared to NN
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]

We get the criterion(ALC) by minimizing the Emperical Bayes mean-square prediction error (MSPE): \\
\begin{align}
 J(x_{j+1}, x) &= E \{ [ Y(x) - \mu_{j+1}(x|D_{j+1}, \hat{\theta}_{j+1}) ] ^2| D_{j}(x) \} \\
 \mu_{j+1}(x|D_{j+1},\hat{\theta}_{j+1}) &= k^{T}(x)K^{-1}Y \nonumber \\
\end{align}  
where $j < n$, $\hat{\theta}_{j+1}$ is the est of $\theta$ on $D_{j+1}$, and $k^{T}(x)$ is the $N$-vector whose $i_{th}$ component is $K_{\theta}(x,x_i)$, $K = K_{\theta}(x_i,x_j)$ \\~\\

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\begin{align}
J(x_{j+1}, x) \approx V_{j}(x|x_{j+1}, \hat{\theta}_{j}) + \left( \frac{\delta \mu_{j}(x;\theta)}{\delta \theta} \Big|_{\theta = \hat{\theta}_j} \right) ^2 / \mathcal{G}_{j+1}(\hat{\theta}_j) \label{eq:papep4}
\end{align}

This term is the expected future inverse Fisher's information:
\begin{align*}
\mathcal{G}_{j+1}(\hat{\theta}_j) &= -\ell''(Y_j;\theta) + E \{ - \frac{\partial^2 \ell_j (y_{j+1};\theta)}{\partial \theta^2} \shortmid D_{j}; \theta \} \\~\\
\end{align*}

This term is the estimate of the predictive variance at $X$ after $X_{j+1}$ is added into the design:
\begin{align*}
V_j(x|x_{j+1};\theta) &= \frac{(j+1)\psi_j}{j(j-1)}\upsilon_{j+1}(x_j; \theta) \\
%
\text{where }\upsilon_{j+1}(x_j; \theta) &= \big[ K_{j+1}(x,x) - k^{T}_{j+1}(x)K^{-1}_{j+1}k^{T}_{j+1}(x) \big] \\
%
\psi &= Y^{T}K^{-1}Y \\
\end{align*}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
The first part of the Eq \ref{eq:papeq4} estimates the predictive variance at $x$ after we include $x_{j+1}$ in the design and the second part estimates the rate of change in the predictive mean at $x$ weighted by the expectation of the future inverse information after $x_{j+1}$ and $y_{j+1}$ are added to the design. \\~\\

Actually, minimizing the first term of Eq \ref{eq:4} is sufficient to minimize the whole equation. This is because:
\begin{itemize}
\item Found the contribution of second term to be small
\item Including it led to smaller designs
\item Computationally, a lot quicker since no derivatives \\~\\
\end{itemize}

So we just need to maximize:
\begin{align}
\upsilon_{j}(x; \theta) - \upsilon_{j+1}(x; \theta)
\end{align}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Putting it all together, we get this algorithm:
\begin{enumerate}
\item Choose starting global $\theta_x = \theta_0$ for all x
\item Calculate local design $X_n(x, \theta_x)$ based on ALC, for each x (independently)
 \begin{enumerate}[(a)]
 \item Choose NN design, $X_{n_0}(x)$ of size $n_0$
 \item For $j = n_0,\ldots,n-1$, set
 \begin{align*}
 x_{j+1} = \text{arg } \max\limits_{x_{j+1} \in X_N \backslash X_j(x)} \upsilon_{j}(x; \theta) - \upsilon_{j+1}(x; \theta),\\
 \text{then update } D_{j+1}(x,\theta_x) = D_j(x,\theta_x) \cup (x_{j+1}, y(x_{j+1}))
 \end{align*}
 \end{enumerate}
\item Independently, calculate $\hat{\theta}_n(x)|D_n(x,\theta_x$). Set $\theta_x = \hat{\theta}_n(x)$
\item Repeat steps 2-3
\item Output predictions $Y(x)|D_n(x,\theta_x)$
\end{enumerate}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
From the algorithm outline, we can see that step 2.b. is the hardest to compute. This is where using the GPU comes in! \\~\\

Note that each candidate's ($x_{j+1}$) calculations can computed independent of each other. This allows us to give each block it's own candidate. From there we give $j$ threads their own sequence of calculations.
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
Here are some simulation results.\\

\begin{figure}[h]
\centering
\includegraphics[height=0.65\textheight]{gpucpucomparepap2.png}
\caption{This graphics compares CPU-only $\&$ GPU-only times for ALC calculations at $n_{cand}=60000$ ($\sim$ max number of blocks) candidate locations while changing $n$ (sample size).}
\end{figure}
\end{frame}





%------------------------------------------------
\subsection{Using own GPU}
%------------------------------------------------

\begin{frame}[fragile]
The writers of the last paper created a \verb|R| package \verb|laGP| in order to easily implement the CUDA, C, and R subroutines. \\~\\

<<exfunct, echo=FALSE>>=
library(xtable)
library(animation)
## examining a particular laGP call from the larger analysis provided
## in the aGP documentation
## Simple 2-d test function used in Gramacy & Apley (2014);
## thanks to Lee, Gramacy, Taddy, and others who have used it before
f2d <- function(x, y=NULL)
{
if(is.null(y)) {
if(!is.matrix(x)) x <- matrix(x, ncol=2)
y <- x[,2]; x <- x[,1]
}
  g <- function(z)
return(exp(-(z-1)^2) + exp(-0.8*(z+1)^2) - 0.05*sin(8*(z+0.1)))
z <- -g(x)*g(y)
}
@

\begin{verbatim}
laGP(Xref, start, end, X, Z, d = NULL, g = 1/10000,
     method = c("alc", "alcray", "mspe", "nn", "fish"), 
     Xi.ret = TRUE,
     close = min(1000*if(method == "alcray") 10 else 1, nrow(X)), 
     alc.gpu = FALSE, numrays = ncol(X), 
     rect = NULL, verb = 0)
\end{verbatim}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
<<ex, echo=TRUE, cache=TRUE>>=
library(laGP)
## build up a design with N=~40K locations
x <- seq(-2, 2, by=0.02)
X <- as.matrix(expand.grid(x, x))
Z <- f2d(X)
## local analysis, first pass
Xref <- matrix(c(-1.725, 1.725), nrow=TRUE)
out <- laGP(Xref, 6, 100, X, Z, method="nn")
## second and pass via ALC, MSPE, and ALC-ray respectively
out2 <- laGP(Xref, 6, 100, X, Z, d=out$mle$d, method="alc", alc.gpu=TRUE)
out2.alccpu <- laGP.R(Xref, 6, 100, X, Z, d=out2$mle$d, method="alc", parallel="none")
out2.alcgpu <- laGP.R(Xref, 6, 100, X, Z, d=out2$mle$d, method="alc", parallel="gpu")
@



<<exTimes, echo=FALSE, results='asis'>>=
## compare times
xtable(data.frame(nn=out$time,
                         alc=out2$time,
                         alcCPU=out2.alccpu$time,
                         alcGPU=out2.alcgpu$time))
@
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
<<exgif, echo=FALSE, eval=FALSE>>=
saveGIF({
  for(i in 1:100){
    plot(rbind(X[out2$Xi,], X[out2$Xi,]), type="n",
         xlab="x1", ylab="x2", main="comparing local designs")
    points(Xref[1], Xref[2], col=2, cex=0.5)
    text(X[out$Xi,], labels=1:100, cex=0.6)
    text(X[out2$Xi[c(1:i)],1],X[out2$Xi[c(1:i)],2], labels=c(1:i), cex=0.6, col="red")
    legend("topright", c("NN", "ALC"),
           text.col=c("black", "red"), bty="n")
  }
}, interval = 0.1, movie.name = "ALC.gif")
@

<<explot, echo=FALSE, fig.height=5, fig.width=7>>=
## look at the different designs
plot(rbind(X[out2$Xi,], X[out2$Xi,]), type="n",
     xlab="x1", ylab="x2", main="comparing local designs")
points(Xref[1], Xref[2], col=2, cex=0.5)
text(X[out$Xi,], labels=1:100, cex=0.6)
text(X[out2$Xi,], labels=1:100, cex=0.6, col="red")
legend("topright", c("NN", "ALC"),
       text.col=c("black", "red"), bty="n")
@
\end{frame}

%------------------------------------------------

\begin{frame}
<<SimStudyplot1, echo=FALSE, fig.width=7, fig.height=5>>=
load("SimRes.RData")
dat <- melt(SimRes)
ggplot(data=subset(dat,Var2==c("theta", "nnThetaEst", "alcThetaEst")), aes(x=Var1, y=value, color=Var2)) + geom_line()
@
\end{frame}

%------------------------------------------------

\begin{frame}
<<SimStudyplot2, echo=FALSE, fig.width=7, fig.height=5>>=
ggplot(data=subset(dat,Var2==c("truepred", "nnPred", "alcPred")), aes(x=Var1, y=value, color=Var2)) + geom_line()
@
\end{frame}

%------------------------------------------------
\section{Discussion}
%------------------------------------------------

\begin{frame}
\begin{itemize}
  \item These two methods are contradicting ideas
  \begin{itemize}
    \item One method says that points nearest are most important
    \item One method says that you need to include some points outside of the nearest points
  \end{itemize}
  \item It is not clear how much better it is to use ALC vs NN
  \item \bm{ADD MORE HERE}
\end{frame}
\end{frame}


\end{document}